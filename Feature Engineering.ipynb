{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#General-information\" data-toc-modified-id=\"General-information-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><strong>General information</strong></a></span></li><li><span><a href=\"#Notebook-setup\" data-toc-modified-id=\"Notebook-setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><strong>Notebook setup</strong></a></span></li><li><span><a href=\"#Feature-engineering-functions\" data-toc-modified-id=\"Feature-engineering-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><strong>Feature engineering functions</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Reading-data\" data-toc-modified-id=\"Reading-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><strong>Reading data</strong></a></span></li><li><span><a href=\"#Filter-instalation_id-with-no-assessments\" data-toc-modified-id=\"Filter-instalation_id-with-no-assessments-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span><strong>Filter <code>instalation_id</code> with no assessments</strong></a></span></li><li><span><a href=\"#Encoding-text-data\" data-toc-modified-id=\"Encoding-text-data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span><strong>Encoding text data</strong></a></span></li><li><span><a href=\"#Determining-start-and-final-events-for-each-training-instance\" data-toc-modified-id=\"Determining-start-and-final-events-for-each-training-instance-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span><strong>Determining start and final events for each training instance</strong></a></span></li><li><span><a href=\"#Generate-missing-assessment-labels\" data-toc-modified-id=\"Generate-missing-assessment-labels-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span><strong>Generate missing assessment labels</strong></a></span></li><li><span><a href=\"#Get-general-information-about-the-performance-of-children-in-each-assessment\" data-toc-modified-id=\"Get-general-information-about-the-performance-of-children-in-each-assessment-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span><strong>Get general information about the performance of children in each assessment</strong></a></span></li><li><span><a href=\"#Obtaining-features-for-each-answered-assessment-(from-both-train-and-test-sets)\" data-toc-modified-id=\"Obtaining-features-for-each-answered-assessment-(from-both-train-and-test-sets)-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span><strong>Obtaining features for each answered assessment (from both train and test sets)</strong></a></span></li><li><span><a href=\"#Generating-the-final-train-and-test-sets\" data-toc-modified-id=\"Generating-the-final-train-and-test-sets-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span><strong>Generating the final train and test sets</strong></a></span></li><li><span><a href=\"#Saving-final-train-and-test-sets\" data-toc-modified-id=\"Saving-final-train-and-test-sets-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span><strong>Saving final train and test sets</strong></a></span></li></ul></li><li><span><a href=\"#Final-dataset-generation\" data-toc-modified-id=\"Final-dataset-generation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><strong>Final dataset generation</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#train_labels_full-generation\" data-toc-modified-id=\"train_labels_full-generation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>train_labels_full generation</a></span></li><li><span><a href=\"#X_train,-Y_train,-X_test-generation\" data-toc-modified-id=\"X_train,-Y_train,-X_test-generation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>X_train, Y_train, X_test generation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **General information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is meant to perform the necessary feature engineering functions in order to condition the training and test data for its input to the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T20:12:58.591608Z",
     "start_time": "2019-12-08T20:12:58.581673Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from numba import jit #high performance python compiler\n",
    "from tqdm.notebook import tqdm #fast, extensible progress bar\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import Parallel, delayed #provides lightweight pipelining in Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "#Python Standard Library\n",
    "import os #miscellaneous operating system interfaces\n",
    "import copy #generic shallow and deep copy operations\n",
    "import gc #interface to the optional garbage collector\n",
    "import time #time access and conversions\n",
    "import datetime #basic date and time types\n",
    "import json #JSON encoder and decoder\n",
    "import re #provides regular expression matching operations\n",
    "from typing import Any, List #support for type hints\n",
    "import warnings #warning control\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from itertools import product #cartesian product, equivalent to a nested for-loop\n",
    "from collections import Counter, defaultdict #Counter: dict subclass for counting hashable objects\n",
    "                                             #defaultdict: dict subclass that calls a factory function to supply missing values\n",
    "pd.options.display.precision = 15\n",
    "pd.set_option('max_rows', 500)\n",
    "    \n",
    "#Others\n",
    "from IPython.display import HTML #Create a display object given raw data\n",
    "import networkx as nx #creation, manipulation, and study of complex networks\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import altair as alt #declarative statistical visualization library based on Vega\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "my_pal = sns.color_palette(n_colors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature engineering functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:05.059614Z",
     "start_time": "2019-12-01T06:24:05.053248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from CSV files\n",
    "def read_data(read_train_labels_full = True):\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('train.csv')\n",
    "    print('Training.csv file has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    print('Test.csv file has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    if read_train_labels_full:\n",
    "        print('Reading train_labels_full.csv file....')\n",
    "        train_labels = pd.read_csv('clean_data/train_labels_full.csv')\n",
    "        print('Train_labels_full.csv file has {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "    else:\n",
    "        print('Reading train_labels.csv file....')\n",
    "        train_labels = pd.read_csv('train_labels.csv')\n",
    "        print('Train_labels.csv file has {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "        \n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('specs.csv')\n",
    "    print('Specs.csv file has {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    ss = pd.read_csv('sample_submission.csv')\n",
    "    print('Sample_submission.csv file has {} rows and {} columns\\n'.format(ss.shape[0], ss.shape[1]))\n",
    "    \n",
    "    return train, test, train_labels, specs, ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filter ``instalation_id`` with no assessments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:05.229358Z",
     "start_time": "2019-12-01T06:24:05.225695Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_installation_ids_with_assessment(train):\n",
    "    \"\"\"All 'installation_id' from the test set have at least one assessment,\n",
    "    but in the train set some don't have any.\n",
    "    \"\"\"\n",
    "    print('Filtering \\'installation_id\\' without assessments....')\n",
    "    installation_id_with_assesment = train.groupby('installation_id')[['type']].agg(lambda x: np.sum(x == 'Assessment') > 0).type.to_dict()\n",
    "    train = train[train['installation_id'].map(installation_id_with_assesment)].reset_index(drop=True)\n",
    "    print('train has {} rows and {} columns\\n'.format(train.shape[0], train.shape[1]))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Encoding text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:05.447596Z",
     "start_time": "2019-12-01T06:24:05.436224Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_text_data(train, test):\n",
    "    print('Encoding text data....\\n')\n",
    "    #Make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_title = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n",
    "    #Make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n",
    "    #Make a list with all the unique 'event_id' from the train and test set\n",
    "    list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n",
    "    #Make a list with all the unique 'world' from the train and test set\n",
    "    list_of_world = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n",
    "    short_list_of_world = list_of_world.copy()\n",
    "    short_list_of_world.pop(2)\n",
    "    #Make a list with all the 'title' that are assessments\n",
    "    list_of_assessment_title = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n",
    "    #Make a list with all the 'game_session' that are assessments\n",
    "    list_of_assessment_session = sorted(list(set(train[train['type'] == 'Assessment']['game_session'].unique()).union(set(test[test['type'] == 'Assessment']['game_session'].unique()))))\n",
    "    \n",
    "    #Create a dictionary encoding the 'title' \n",
    "    title_map = dict(zip(list_of_title, np.arange(len(list_of_title))))    #(keys: str | values: int)\n",
    "    title_map_labels = dict(zip(np.arange(len(list_of_title)), list_of_title)) #reversed dictionary (keys: int | values: str)\n",
    "    #Create a dictionary encoding the 'world' \n",
    "    world_map = dict(zip(list_of_world, np.arange(len(list_of_world))))\n",
    "    \n",
    "    #Replace the text data with the numerical data from the dictionaries\n",
    "    train['title'] = train['title'].map(title_map)\n",
    "    test['title'] = test['title'].map(title_map)\n",
    "    train['world'] = train['world'].map(world_map)\n",
    "    test['world'] = test['world'].map(world_map)\n",
    "    \n",
    "    #Create a dictionary with the 'event_code' that has the information of the correct answers\n",
    "    correct_event_code_map = dict(zip(title_map.values(), (4100*np.ones(len(title_map))).astype('int')))\n",
    "    correct_event_code_map[title_map['Bird Measurer (Assessment)']] = 4110 #Change the one for 'Bird Measurer (Assessment)' to 4110\n",
    "    \n",
    "    #Convert 'timestamp' into datetime class\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    return train, test, correct_event_code_map, list_of_title, list_of_world, short_list_of_world,list_of_event_code, title_map, title_map_labels, world_map, list_of_assessment_title, list_of_assessment_session, list_of_event_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Determining start and final events for each training instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:05.608715Z",
     "start_time": "2019-12-01T06:24:05.600032Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_start_and_final_events(train,test):\n",
    "    print('Determining start and final events for each training instance....\\n')\n",
    "    # Sort values\n",
    "    train = train.sort_values(by=['installation_id', 'timestamp'])\n",
    "    test = test.sort_values(by=['installation_id', 'timestamp'])\n",
    "    \n",
    "    # Identify start events\n",
    "    train_min = train.groupby('installation_id')[['timestamp']].min()\n",
    "    train_min['initial_time'] = 1\n",
    "    train_min = train_min.reset_index()\n",
    "    train = train.merge(train_min, on=['installation_id', 'timestamp'], how='left')\n",
    "    train['initial_time'].fillna(0, inplace=True)\n",
    "    \n",
    "    test_min = test.groupby('installation_id')[['timestamp']].min()\n",
    "    test_min['initial_time'] = 1\n",
    "    test_min = test_min.reset_index()\n",
    "    test = test.merge(test_min, on=['installation_id', 'timestamp'], how='left')\n",
    "    test['initial_time'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Identify final events (first event of each assessment)\n",
    "    train_max = train[train['type'] == 'Assessment']\n",
    "    train_max = train_max.groupby('game_session')[['timestamp']].min()\n",
    "    train_max['final_time'] = 1\n",
    "    train_max = train_max.reset_index()\n",
    "    train = train.merge(train_max, on=['game_session', 'timestamp'], how='left')\n",
    "    train['final_time'].fillna(0, inplace=True)\n",
    "    \n",
    "    test_max = test[test['type'] == 'Assessment']\n",
    "    test_max = test_max.groupby('game_session')[['timestamp']].min()\n",
    "    test_max['final_time'] = 1\n",
    "    test_max = test_max.reset_index()\n",
    "    test = test.merge(test_max, on=['game_session', 'timestamp'], how='left')\n",
    "    test['final_time'].fillna(0, inplace=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate missing assessment labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:05.778246Z",
     "start_time": "2019-12-01T06:24:05.766767Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_assessment_accuracy(df,assessment_session):\n",
    "    assessment_df = df[df['game_session']== assessment_session]\n",
    "    assessment_info = {'game_session' : assessment_session}\n",
    "    assessment_info.update({'installation_id' : assessment_df.iloc[-1].installation_id})\n",
    "    assessment_title = assessment_df.iloc[-1].title\n",
    "    assessment_info.update({'title' : title_map_labels[assessment_title]})\n",
    "    \n",
    "    #accumulated_correct_attempts & accumulated_incorrect_attempts\n",
    "    all_attempts = assessment_df.query(f'event_code == {correct_event_code_map[assessment_title]}')\n",
    "    assessment_info.update({'num_correct' : all_attempts['event_data'].str.contains('true').sum()})\n",
    "    assessment_info.update({'num_incorrect' : all_attempts['event_data'].str.contains('false').sum()})\n",
    "    assessment_info.update({'accuracy' : assessment_info['num_correct']/(assessment_info['num_correct']+assessment_info['num_incorrect']) if (assessment_info['num_correct']+assessment_info['num_incorrect']) != 0 else 0})\n",
    "    #accuracy_group\n",
    "    if assessment_info['accuracy'] == 0:\n",
    "        assessment_info.update({'accuracy_group' : 0})\n",
    "    elif assessment_info['accuracy'] == 1:\n",
    "        assessment_info.update({'accuracy_group' : 3})\n",
    "    elif assessment_info['accuracy'] == 0.5:\n",
    "        assessment_info.update({'accuracy_group' : 2})\n",
    "    else:\n",
    "        assessment_info.update({'accuracy_group' : 1})\n",
    "        \n",
    "    return assessment_info\n",
    "    \n",
    "def get_train_labels_full(train_labels,train,test):\n",
    "    print('Generating missing assessment labels...')\n",
    "    train_labels_full = train_labels\n",
    "    list_of_session_with_label = train_labels['game_session'].unique()\n",
    "    print('{} sessions with labels: {}...'.format(len(list_of_session_with_label),list_of_session_with_label[:5]))\n",
    "    list_of_session_without_label = np.setdiff1d(list_of_assessment_session,list_of_session_with_label)\n",
    "    print('{} sessions without labels: {}...'.format(len(list_of_session_without_label),list_of_session_without_label[:5]))\n",
    "    list_of_sessions_to_be_predicted = []\n",
    "    for assessment_session in tqdm(list_of_session_without_label, total= 6896):\n",
    "        if train[train['game_session'] == assessment_session].any(axis=None):\n",
    "            train_labels_full = train_labels_full.append(get_assessment_accuracy(train,assessment_session),ignore_index=True)\n",
    "        else:\n",
    "            if test[test['game_session'] == assessment_session].shape[0] > 1:\n",
    "                train_labels_full = train_labels_full.append(get_assessment_accuracy(test,assessment_session),ignore_index=True)\n",
    "            else:\n",
    "                list_of_sessions_to_be_predicted.append(assessment_session)\n",
    "                \n",
    "    print('{} sessions to be predicted by model: {}...'.format(len(list_of_sessions_to_be_predicted),list_of_sessions_to_be_predicted[:5]))\n",
    "    print('Result: {} labels generated\\n'.format(train_labels_full.shape[0]-train_labels.shape[0]))\n",
    "    train_labels_full.to_csv('train_labels_full.csv', index=False)\n",
    "    return train_labels_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get general information about the performance of children in each assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:05.937646Z",
     "start_time": "2019-12-01T06:24:05.932342Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_assessment_info(train_labels_full):\n",
    "    \"\"\"Gets the mean and median of both accuracy and accuracy_group per each assessment\n",
    "    \"\"\"\n",
    "    print('Getting general information about the performance of children in each assessment...\\n')\n",
    "    mean_acc_map = {title: train_labels_full[train_labels_full['title'] == title]['accuracy'].mean() for title in list_of_assessment_title}\n",
    "    mean_acc_group_map = {title: train_labels_full[train_labels_full['title'] == title]['accuracy_group'].mean() for title in list_of_assessment_title}\n",
    "    \n",
    "    median_acc_map = {title: train_labels_full[train_labels_full['title'] == title]['accuracy'].median() for title in list_of_assessment_title}\n",
    "    median_acc_group_map = {title: train_labels_full[train_labels_full['title'] == title]['accuracy_group'].median() for title in list_of_assessment_title}\n",
    "    \n",
    "    return mean_acc_map, mean_acc_group_map, median_acc_map, median_acc_group_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Obtaining features for each answered assessment (from both train and test sets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:06.125934Z",
     "start_time": "2019-12-01T06:24:06.100620Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_correct_pct(series, subs=False):\n",
    "    tot, num_cor = 0, 0\n",
    "    for s in series:\n",
    "        dict_s = json.loads(s)\n",
    "        if 'correct' in dict_s.keys():\n",
    "            tot += 1\n",
    "            if dict_s['correct']:\n",
    "                num_cor += 1\n",
    "    if subs and num_cor > 0:\n",
    "        num_cor -= 1\n",
    "        tot -= 1\n",
    "    return num_cor / tot if tot > 0 else 0.0\n",
    "\n",
    "def observation(df, k, i,test_set=False):\n",
    "    # Select the time series corresponding to installation_id k and Assessment i\n",
    "    df2 = df[(df['final_time_sum'] <= i) | (df['final_time_sum'] == i + 1) & (df['final_time'] == 1)]\n",
    "    df = df2.reset_index(drop=True)\n",
    "    df.loc[:, 'date'] = df.loc[:, 'timestamp'].dt.date\n",
    "    \n",
    "    #Getting the features for the assessment\n",
    "    features = {}\n",
    "    #assesment dependent features\n",
    "    assessment_row = df.iloc[-1]\n",
    "    features.update({'installation_id': assessment_row['installation_id']})\n",
    "    features.update({'game_session': assessment_row['game_session']})\n",
    "    features.update({'is_' + assessment_title.replace(' ', '_')[:-13]: 1 if assessment_row['title'] == title_map[assessment_title] else 0 for assessment_title in list_of_assessment_title})\n",
    "    features.update({'is_' + world: 1 if assessment_row['world'] == world_map[world] else 0 for world in short_list_of_world})\n",
    "    features.update({'hour': assessment_row['timestamp'].hour})\n",
    "    features.update({'dayofweek': assessment_row['timestamp'].dayofweek})\n",
    "    \n",
    "    features.update({'num_unique_dates': df['date'].nunique()})\n",
    "    features.update({'mean_acc': mean_acc_map[title_map_labels[assessment_row['title']]]})\n",
    "    features.update({'mean_acc_group': mean_acc_group_map[title_map_labels[assessment_row['title']]]})\n",
    "    features.update({'median_acc': median_acc_map[title_map_labels[assessment_row['title']]]})\n",
    "    features.update({'median_acc_group': median_acc_group_map[title_map_labels[assessment_row['title']]]})\n",
    "    \n",
    "    #counter features\n",
    "    #title_count\n",
    "    title_count = df.groupby('game_session')['title'].max().value_counts().to_dict()\n",
    "    for title in title_map.values():\n",
    "        features.update({'num_' + str(title): title_count[title] if title in title_count else 0})\n",
    "    features['num_'+ str(assessment_row['title'])] -= 1\n",
    "    features.update({'num_total_title': sum(title_count.values())})\n",
    "    features.update({'previous_completions': features['num_'+ str(assessment_row['title'])]})\n",
    "    \n",
    "    #title_type_count\n",
    "    title_type_count = df.groupby('game_session')['type'].max().value_counts().to_dict()\n",
    "    title_type_keys = ['Assessment', 'Game', 'Clip', 'Activity']\n",
    "    for title_type in title_type_keys:\n",
    "        features.update({'num_' + title_type: title_type_count[title_type] if title_type in title_type_count else 0})\n",
    "    features['num_Assessment'] -= 1\n",
    "    \n",
    "    #title_type_same_world_count\n",
    "    title_type_same_world_count = df[df.world == assessment_row['world']].groupby('game_session')['type'].max().value_counts().to_dict()\n",
    "    for title_type in title_type_keys:\n",
    "        features.update({'num_' + title_type + '_same_world': title_type_same_world_count[title_type] if title_type in title_type_same_world_count else 0})\n",
    "    features['num_Assessment_same_world'] -= 1\n",
    "    features.update({'num_total_title_same_world': sum(title_type_same_world_count.values())})\n",
    "    \n",
    "    #event_code_count\n",
    "    event_code_count = df.groupby('event_code')['event_code'].count().to_dict()\n",
    "    for event_code in list_of_event_code:\n",
    "        features.update({str(event_code): event_code_count[event_code] if event_code in event_code_count else 0})\n",
    "    features.update({'total_event_count': sum(event_code_count.values())})             \n",
    "    \n",
    "    #min_count\n",
    "    min_count_by_type = df.groupby('type')['game_time'].agg(lambda x: sum(x)/(60 * 1000)).to_dict()\n",
    "    for title_type in title_type_keys:\n",
    "        if title_type == 'Clip':\n",
    "            pass\n",
    "        else:\n",
    "            features.update({'mins_' + title_type: min_count_by_type[title_type] if title_type in min_count_by_type else 0})\n",
    "    features.update({'mins_total': sum(min_count_by_type.values())})\n",
    "    \n",
    "    #avg_min_count\n",
    "    avg_min_count_by_type = df.groupby('type')['game_time'].agg(lambda x: sum(x)/(60 * 1000)).to_dict()\n",
    "    for title_type in title_type_keys:\n",
    "        if title_type == 'Clip':\n",
    "            pass\n",
    "        else:\n",
    "            features.update({'avg_mins_' + title_type: avg_min_count_by_type[title_type]/features['num_' + title_type] if title_type in min_count_by_type and features['num_' + title_type]!= 0 else 0})\n",
    "    features.update({'avg_mins_total': sum(avg_min_count_by_type.values())/features['num_total_title']})\n",
    "    \n",
    "    #correct_pct's\n",
    "    game_pct = df[df['type'] == 'Game'].groupby('game_session')['event_data'].agg(lambda x: extract_correct_pct(x)).mean()\n",
    "    features.update({'game_pct': game_pct if game_pct is not np.nan else 0})\n",
    "    ass_pct = df[df['type'] == 'Assessment'].groupby('game_session')['event_data'].agg(lambda x: extract_correct_pct(x,True)).mean()\n",
    "    features.update({'ass_pct': ass_pct if ass_pct is not np.nan else 0})\n",
    "    \n",
    "    #accumulated_correct/incorrect_attemps and accuracies\n",
    "    accumulated_correct_attempts = 0\n",
    "    accumulated_incorrect_attempts = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_accuracy_group = 0\n",
    "    last_accuracy_same_title = -1\n",
    "    last_accuracy_group_same_title = -1\n",
    "    \n",
    "    for assessment_session in df[df['type'] == 'Assessment']['game_session'].unique():\n",
    "        if assessment_session == features['game_session']: #skipping the assessment to be labeled\n",
    "            pass\n",
    "        else:\n",
    "            accumulated_correct_attempts += train_labels_full[train_labels_full['game_session'] == assessment_session]['num_correct'].values[0]\n",
    "            accumulated_incorrect_attempts += train_labels_full[train_labels_full['game_session'] == assessment_session]['num_incorrect'].values[0]\n",
    "            accuracy = train_labels_full[train_labels_full['game_session'] == assessment_session]['accuracy'].values[0]\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_same_title = accuracy if title_map[train_labels_full[train_labels_full['game_session'] == assessment_session]['title'].values[0]] == assessment_row['title'] else last_accuracy_same_title \n",
    "            accuracy_group = train_labels_full[train_labels_full['game_session'] == assessment_session]['accuracy_group'].values[0]\n",
    "            accumulated_accuracy_group +=  accuracy_group\n",
    "            last_accuracy_group_same_title = accuracy_group if title_map[train_labels_full[train_labels_full['game_session'] == assessment_session]['title'].values[0]] == assessment_row['title'] else last_accuracy_group_same_title \n",
    "            \n",
    "    features.update({'accumulated_correct_attempts': accumulated_correct_attempts})\n",
    "    features.update({'accumulated_incorrect_attempts': accumulated_incorrect_attempts})\n",
    "    features.update({'accumulated_accuracy': accumulated_accuracy})\n",
    "    features.update({'accumulated_accuracy_group': accumulated_accuracy_group/features['num_Assessment'] if features['num_Assessment'] > 0 else 0})\n",
    "    features.update({'last_accuracy_same_title': last_accuracy_same_title})\n",
    "    features.update({'last_accuracy_group_same_title': last_accuracy_group_same_title})\n",
    "    \n",
    "    #Getting the label for the assessment if it is not an incomplete one from the test set\n",
    "    if not test_set:\n",
    "        assessment_accuracy = features.update({'accuracy_group': train_labels_full[train_labels_full['game_session'] == assessment_session]['accuracy_group'].values[0]})\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating the final train and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:06.274669Z",
     "start_time": "2019-12-01T06:24:06.263668Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_final_data(train, test):\n",
    "    train['final_time_sum'] = train['final_time'].groupby(train['installation_id']).transform('cumsum')\n",
    "    test['final_time_sum'] = test['final_time'].groupby(test['installation_id']).transform('cumsum')\n",
    "    obs_per_id_train = train.groupby('installation_id')[['final_time_sum']].max().astype('int32').to_dict()['final_time_sum']\n",
    "    obs_per_id_test = test.groupby('installation_id')[['final_time_sum']].max().astype('int32').to_dict()['final_time_sum']\n",
    "    final_train = []\n",
    "    final_test = []    \n",
    "    print('Obtaining train features:')\n",
    "    #final_train = parallelize_dataframe(obs_list,final_train,n_cores = 4)\n",
    "    for k, v in tqdm(obs_per_id_train.items(),total= 4242):\n",
    "        df_ = train[train['installation_id'] == k]\n",
    "        for i in range(v):\n",
    "            final_train.append(observation(df_, k, i))\n",
    "\n",
    "    print('Obtaining test features:')\n",
    "    for k, v in tqdm(obs_per_id_test.items(), total = 1000):\n",
    "        df_ = test[test['installation_id'] == k]\n",
    "        for i in range(v-1):\n",
    "            final_train.append(observation(df_, k, i))\n",
    "        final_test.append(observation(df_, k, v - 1,test_set=True))\n",
    "    \n",
    "    final_train = pd.DataFrame(final_train)\n",
    "    final_train = final_train.reset_index(drop=True)\n",
    "    X_train = final_train.iloc[:, 2:-1]\n",
    "    Y_train = final_train.iloc[:, -1]\n",
    "    \n",
    "    final_test = pd.DataFrame(final_test)\n",
    "    final_test = final_test.reset_index(drop=True)\n",
    "    X_test = final_test.iloc[:, 2:]\n",
    "    \n",
    "    print('Final train (\\'X_train\\') information:\\nN = {}\\np= {}\\nsum(y)={}\\n\\n'.format(X_train.shape[0],X_train.shape[1],Y_train.shape[0]))\n",
    "    print('Final test (\\'X_test\\') information:\\nN = {}\\np= {}'.format(X_test.shape[0],X_test.shape[1]))\n",
    "    \n",
    "    return X_train, Y_train, X_test, final_train, final_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving final train and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:06.438730Z",
     "start_time": "2019-12-01T06:24:06.435457Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_data(X_train, Y_train, X_test,final_train,final_test):\n",
    "    X_train.to_csv('clean_data/X_train.csv', index=False)\n",
    "    Y_train.to_csv('clean_data/Y_train.csv', index=False)\n",
    "    X_test.to_csv('clean_data/X_test.csv', index=False)\n",
    "    final_train.to_csv('clean_data/final_train.csv', index=False)\n",
    "    final_test.to_csv('clean_data/final_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final dataset generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_labels_full generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T06:24:06.812052Z",
     "start_time": "2019-12-01T06:24:06.810221Z"
    }
   },
   "outputs": [],
   "source": [
    "#If train_labels_full hasn't still been generated, run this cell\n",
    "\n",
    "#Read data\n",
    "#train, test, train_labels, specs, ss = read_data(read_train_labels_full = False)\n",
    "#Filter 'installation_id' without assements from train\n",
    "#train = get_installation_ids_with_assessment(train)\n",
    "#Get useful dictionaries with encoded titles, worlds, types...\n",
    "#train, test, correct_event_code_map, list_of_title, list_of_world, short_list_of_world, list_of_event_code, title_map, title_map_labels, world_map, list_of_assessment_title, list_of_assessment_session, list_of_event_id = encode_text_data(train, test)\n",
    "#Generate missing assessment labels\n",
    "#train_labels_full = get_train_labels_full(train_labels,train,test)\n",
    "#Storing the new data set\n",
    "#train_labels_full.to_csv('clean_data/train_labels_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, Y_train, X_test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T07:35:33.508243Z",
     "start_time": "2019-12-01T06:24:12.250809Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "train, test, train_labels_full, specs, ss = read_data()\n",
    "#Filter 'installation_id' without assements from train\n",
    "train = get_installation_ids_with_assessment(train)\n",
    "#Get useful dictionaries with encoded titles, worlds, types...\n",
    "train, test, correct_event_code_map, list_of_title, list_of_world,short_list_of_world, list_of_event_code, title_map, title_map_labels, world_map, list_of_assessment_title, list_of_assessment_session, list_of_event_id = encode_text_data(train, test)\n",
    "#Identify start and final events for each assessment in both the train and test sets\n",
    "train, test= identify_start_and_final_events(train,test)\n",
    "#Get general metrics about the performance of children in each assessment\n",
    "mean_acc_map, mean_acc_group_map, median_acc_map, median_acc_group_map = get_assessment_info(train_labels_full)\n",
    "#Generate final training and test set\n",
    "X_train,Y_train, X_test, final_train, final_test = get_final_data(train,test)\n",
    "#Saving data \n",
    "save_data(X_train, Y_train, X_test,final_train,final_test)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "220px",
    "width": "560px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
